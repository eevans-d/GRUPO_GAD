# Configuraci贸n de Alertas para GRUPO_GAD

Este documento describe la configuraci贸n de alertas y monitoreo para el sistema GRUPO_GAD en producci贸n. Forma parte de la Fase 6 del roadmap: Post-Producci贸n (Mantenimiento y Fine-Tuning).

## Arquitectura de Monitoreo

El sistema de monitoreo se compone de:

1. **Prometheus** - Recolecci贸n de m茅tricas
2. **Grafana** - Visualizaci贸n de dashboards
3. **Alertmanager** - Gesti贸n y distribuci贸n de alertas
4. **Node Exporter** - M茅tricas del sistema operativo
5. **Redis Exporter** - M茅tricas de Redis
6. **Postgres Exporter** - M茅tricas de PostgreSQL

## M茅tricas Cr铆ticas

### 1. M茅tricas de Aplicaci贸n

| M茅trica | Descripci贸n | Umbral de Alerta |
|---------|-------------|------------------|
| `http_requests_total` | Total de peticiones HTTP | Rate > 1000 req/min |
| `http_request_duration_seconds` | Latencia de peticiones | P95 > 2s |
| `http_requests_errors_total` | Errores HTTP (4xx, 5xx) | Rate > 50 errors/min |
| `websocket_connections_total` | Conexiones WebSocket activas | > 500 conexiones |
| `websocket_messages_sent_total` | Mensajes WebSocket enviados | Rate > 10000 msg/min |
| `database_connections_active` | Conexiones activas a BD | > 80% del pool |
| `redis_connected_clients` | Clientes conectados a Redis | > 100 clientes |

### 2. M茅tricas de Infraestructura

| M茅trica | Descripci贸n | Umbral de Alerta |
|---------|-------------|------------------|
| `up` | Estado del servicio | 0 (servicio ca铆do) |
| `node_cpu_seconds_total` | Uso de CPU | > 85% durante 5min |
| `node_memory_MemAvailable_bytes` | Memoria disponible | < 10% disponible |
| `node_filesystem_avail_bytes` | Espacio en disco | < 10% disponible |
| `node_load15` | Load average 15min | > 2.0 |

### 3. M茅tricas de Base de Datos

| M茅trica | Descripci贸n | Umbral de Alerta |
|---------|-------------|------------------|
| `pg_up` | Estado de PostgreSQL | 0 (BD ca铆da) |
| `pg_stat_database_numbackends` | Conexiones activas | > 50 conexiones |
| `pg_stat_database_tup_inserted` | Rate de inserciones | Cambio > 1000% |
| `pg_database_size_bytes` | Tama帽o de BD | > 80% del espacio asignado |
| `pg_stat_bgwriter_checkpoints_timed` | Checkpoints programados | Rate < 1/min |

## Configuraci贸n de Alertas

### Alertas Cr铆ticas (P0)

```yaml
groups:
- name: critical_alerts
  rules:
  - alert: ServiceDown
    expr: up == 0
    for: 30s
    labels:
      severity: critical
      priority: P0
    annotations:
      summary: "Servicio {{ $labels.instance }} est谩 ca铆do"
      description: "El servicio {{ $labels.job }} en {{ $labels.instance }} no responde."

  - alert: HighErrorRate
    expr: rate(http_requests_errors_total[5m]) > 50
    for: 2m
    labels:
      severity: critical
      priority: P0
    annotations:
      summary: "Tasa de errores alta en API"
      description: "La API est谩 generando {{ $value }} errores por minuto."

  - alert: DatabaseDown
    expr: pg_up == 0
    for: 30s
    labels:
      severity: critical
      priority: P0
    annotations:
      summary: "Base de datos PostgreSQL ca铆da"
      description: "PostgreSQL no est谩 respondiendo en {{ $labels.instance }}."

  - alert: DiskSpaceCritical
    expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 5
    for: 1m
    labels:
      severity: critical
      priority: P0
    annotations:
      summary: "Espacio en disco cr铆ticamente bajo"
      description: "El disco {{ $labels.mountpoint }} tiene menos del 5% de espacio libre."
```

### Alertas de Advertencia (P1)

```yaml
- name: warning_alerts
  rules:
  - alert: HighCPUUsage
    expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
    for: 5m
    labels:
      severity: warning
      priority: P1
    annotations:
      summary: "Uso alto de CPU"
      description: "CPU al {{ $value }}% en {{ $labels.instance }} durante 5 minutos."

  - alert: HighMemoryUsage
    expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
    for: 3m
    labels:
      severity: warning
      priority: P1
    annotations:
      summary: "Uso alto de memoria"
      description: "Memoria al {{ $value }}% en {{ $labels.instance }}."

  - alert: HighResponseTime
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
    for: 3m
    labels:
      severity: warning
      priority: P1
    annotations:
      summary: "Tiempo de respuesta alto"
      description: "P95 de tiempo de respuesta: {{ $value }}s."

  - alert: TooManyDatabaseConnections
    expr: pg_stat_database_numbackends > 50
    for: 2m
    labels:
      severity: warning
      priority: P1
    annotations:
      summary: "Muchas conexiones a la base de datos"
      description: "{{ $value }} conexiones activas a PostgreSQL."
```

## Canales de Notificaci贸n

### 1. Configuraci贸n de Alertmanager

```yaml
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@grupogad.com'
  smtp_auth_username: 'alerts@grupogad.com'
  smtp_auth_password: 'your_app_password'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'
  routes:
  - match:
      priority: P0
    receiver: critical-alerts
  - match:
      priority: P1
    receiver: warning-alerts

receivers:
- name: 'web.hook'
  webhook_configs:
  - url: 'http://127.0.0.1:5001/'

- name: 'critical-alerts'
  email_configs:
  - to: 'admin@grupogad.com'
    subject: '[CRTICO] {{ .GroupLabels.alertname }}'
    body: |
      {{ range .Alerts }}
      Alerta: {{ .Annotations.summary }}
      Descripci贸n: {{ .Annotations.description }}
      Severidad: {{ .Labels.severity }}
      Instancia: {{ .Labels.instance }}
      Tiempo: {{ .StartsAt }}
      {{ end }}
  telegram_configs:
  - api_url: 'https://api.telegram.org'
    bot_token: 'YOUR_TELEGRAM_BOT_TOKEN'
    chat_id: -1001234567890
    message: |
       *ALERTA CRTICA*
      {{ range .Alerts }}
      *{{ .Annotations.summary }}*
      {{ .Annotations.description }}
      Severidad: {{ .Labels.severity }}
      {{ end }}

- name: 'warning-alerts'
  email_configs:
  - to: 'devops@grupogad.com'
    subject: '[ADVERTENCIA] {{ .GroupLabels.alertname }}'
    body: |
      {{ range .Alerts }}
      Alerta: {{ .Annotations.summary }}
      Descripci贸n: {{ .Annotations.description }}
      Severidad: {{ .Labels.severity }}
      {{ end }}
```

### 2. Integraci贸n con Telegram

Para recibir alertas en Telegram:

1. Crear un bot con @BotFather
2. Obtener el token del bot
3. Crear un grupo/canal para alertas
4. A帽adir el bot al grupo y obtener el chat_id
5. Configurar en Alertmanager

### 3. Integraci贸n con Slack (Opcional)

```yaml
- name: 'slack-alerts'
  slack_configs:
  - api_url: 'YOUR_SLACK_WEBHOOK_URL'
    channel: '#alerts'
    title: 'Alerta GRUPO_GAD'
    text: |
      {{ range .Alerts }}
      {{ .Annotations.summary }}
      {{ .Annotations.description }}
      {{ end }}
```

## Dashboards de Grafana

### 1. Dashboard Principal - Overview

M茅tricas incluidas:
- Estado general de servicios
- Tr谩fico HTTP (requests/min, errores, latencia)
- Uso de recursos (CPU, memoria, disco)
- Conexiones WebSocket activas
- Estado de la base de datos

### 2. Dashboard de Infraestructura

M茅tricas incluidas:
- M茅tricas detalladas del sistema operativo
- Uso de red
- I/O de disco
- Procesos en ejecuci贸n
- Load average hist贸rico

### 3. Dashboard de Aplicaci贸n

M茅tricas incluidas:
- M茅tricas espec铆ficas de GRUPO_GAD
- Performance de endpoints
- Patrones de uso
- M茅tricas de negocio

## Plan de Escalado de Alertas

### Nivel 1 - Alerta Inicial
- Notificaci贸n autom谩tica via Telegram/Email
- Registro en sistema de tickets
- Timeout: 5 minutos

### Nivel 2 - Escalado
- Si no hay resoluci贸n en 15 minutos
- Notificaci贸n al equipo de guardia
- Llamada telef贸nica autom谩tica

### Nivel 3 - Escalado Cr铆tico
- Si no hay resoluci贸n en 30 minutos
- Notificaci贸n a management
- Activaci贸n de procedimientos de emergencia

## M茅tricas de SLA

### Objetivos de Nivel de Servicio

| M茅trica | Objetivo | Medici贸n |
|---------|----------|----------|
| Disponibilidad | 99.9% | Uptime mensual |
| Tiempo de respuesta | P95 < 1s | Para endpoints principales |
| Tiempo de resoluci贸n | < 15 min | Para alertas P0 |
| Tiempo de recuperaci贸n | < 5 min | Tras ca铆da de servicio |

### C谩lculo de SLA

```promql
# Disponibilidad (%)
(1 - (sum(rate(up == 0[30d])) / sum(rate(up[30d])))) * 100

# Latencia P95
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[30d]))

# Tasa de errores
rate(http_requests_errors_total[30d]) / rate(http_requests_total[30d]) * 100
```

## Mantenimiento del Sistema de Monitoreo

### Tareas Regulares

1. **Diario:**
   - Revisar dashboards principales
   - Verificar que no hay alertas pendientes
   - Comprobar espacio en disco de m茅tricas

2. **Semanal:**
   - Revisar tendencias de m茅tricas
   - Ajustar umbrales si es necesario
   - Verificar backups de configuraci贸n

3. **Mensual:**
   - Revisar SLA y generar reportes
   - Optimizar consultas lentas
   - Actualizar documentaci贸n de runbooks

### Retenci贸n de Datos

- **M茅tricas alta frecuencia:** 7 d铆as
- **M茅tricas media frecuencia:** 30 d铆as
- **M茅tricas baja frecuencia:** 90 d铆as
- **Alertas hist贸ricas:** 180 d铆as

---

## Referencias

- [Prometheus Alerting Rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/)
- [Alertmanager Configuration](https://prometheus.io/docs/alerting/latest/configuration/)
- [Grafana Dashboards](https://grafana.com/docs/grafana/latest/dashboards/)
- [SLO/SLA Best Practices](https://cloud.google.com/blog/products/management-tools/sre-fundamentals-slis-slas-and-slos)